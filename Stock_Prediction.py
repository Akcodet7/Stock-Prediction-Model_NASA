# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VRw0YNxf2N2OBwTNKR6QqSpSkvbBXhRT
"""

pip install textblob feedparser yfinance

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
import yfinance as yf
import seaborn as sns
import feedparser
from textblob import TextBlob

sns.set_style('whitegrid')

# --- Function to fetch stock-related news using RSS feeds ---
def fetch_news(symbol):
    """
    Fetches stock news for the given symbol using RSS feeds.
    """
    feeds = [
        f'https://feeds.finance.yahoo.com/rss/2.0/headline?s={symbol}&region=US&lang=en-US',
        f'https://seekingalpha.com/api/sa/combined/{symbol}.xml'
    ]

    news_items = []
    for feed in feeds:
        try:
            news = feedparser.parse(feed)
            # Fetch up to 10 entries per feed
            news_items.extend([{
                'title': entry.title,
                'date': pd.to_datetime(entry.published) if 'published' in entry else None
            } for entry in news.entries[:10]])
        except Exception as e:
            print(f"Error fetching from {feed}: {e}")
            continue

    # Save the news results locally
    with open("stock_news.json", "w") as f:
        json.dump(news_items, f, indent=4, default=str)

    return news_items

# --- Function to perform sentiment analysis using TextBlob ---
def analyze_sentiment(news_articles):
    """
    Analyzes the sentiment of news headlines using TextBlob.
    Returns a list of dictionaries with the title, sentiment label, polarity score, and date.
    """
    sentiments = []
    for article in news_articles:
        analysis = TextBlob(article['title'])
        polarity = analysis.sentiment.polarity

        if polarity > 0:
            sentiment = 'positive'
        elif polarity < 0:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'

        sentiments.append({
            "title": article['title'],
            "sentiment": sentiment,
            "polarity": polarity,
            "date": article.get('date', None)
        })

    # Save sentiment results to CSV
    sentiment_df = pd.DataFrame(sentiments)
    sentiment_df.to_csv("stock_sentiment.csv", index=False)
    print("Sentiment analysis saved to stock_sentiment.csv")
    return sentiments

# --- Helper function to create datasets for LSTM ---
def create_dataset(dataset, time_step=100):
    X, y = [], []
    for i in range(len(dataset) - time_step):
        X.append(dataset[i:i+time_step, 0])
        y.append(dataset[i+time_step, 0])
    return np.array(X), np.array(y)

# --- Main function combining all parts ---
def main():
    stock_symbol = "^NSEI"  # Example stock symbol

    # 1. Fetch and Analyze News Sentiment using RSS feeds and TextBlob
    news_data = fetch_news(stock_symbol)
    sentiments = analyze_sentiment(news_data)

    # 2. Download historical stock data from Yahoo Finance.
    df_stock = yf.download(stock_symbol, start='2015-01-01', end='2025-01-01')
    data = df_stock[['Close']]
    print("Stock data shape:", data.shape)

    # 3. Data Scaling
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data)

    # 4. Split Data into Training and Testing Sets
    training_size = int(len(scaled_data) * 0.65)
    train_data = scaled_data[:training_size]
    test_data = scaled_data[training_size:]

    # 5. Create Datasets for the LSTM Model
    time_step = 100
    X_train, y_train = create_dataset(train_data, time_step)
    X_test, y_test = create_dataset(test_data, time_step)

    # Reshape for LSTM input [samples, time steps, features]
    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

    # 6. Build the Stacked LSTM Model
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=(time_step, 1)))
    model.add(LSTM(units=50))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.summary()

    # 7. Train the Model
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=10, batch_size=64, verbose=1)

    # 8. Model Prediction and Evaluation
    train_predict = model.predict(X_train)
    test_predict = model.predict(X_test)

    # Inverse transform predictions and actual values
    train_predict = scaler.inverse_transform(train_predict)
    test_predict = scaler.inverse_transform(test_predict)
    y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))
    y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))

    train_rmse = math.sqrt(mean_squared_error(y_train_actual, train_predict))
    test_rmse = math.sqrt(mean_squared_error(y_test_actual, test_predict))
    print("Train RMSE: {:.2f}".format(train_rmse))
    print("Test RMSE: {:.2f}".format(test_rmse))

    # Judging Criterion Calculation
    test_predict_start = float(test_predict[0])
    test_predict_end = float(test_predict[-1])
    returns = (test_predict_end - test_predict_start) / test_predict_start * 100
    variance = float(np.var(test_predict))
    judging_score = ((returns - 6) / variance) * 100
    print("Returns: {:.2f}%".format(returns))
    print("Variance: {:.2f}".format(variance))
    print("Judging Score: {:.2f}%".format(float(judging_score)))

    # 9. Prepare Data for Plotting Actual vs. Predicted
    train_plot = np.empty_like(scaled_data)
    train_plot[:, :] = np.nan
    train_plot[time_step:len(train_predict)+time_step, 0] = train_predict[:, 0]

    test_plot = np.empty_like(scaled_data)
    test_plot[:, :] = np.nan
    test_plot[training_size+time_step:len(scaled_data), 0] = test_predict[:, 0]

    # 10. Plot Training and Validation Loss
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    # 11. Plot Actual vs. Predicted Stock Prices
    plt.figure(figsize=(14, 7))
    plt.plot(data.index, scaler.inverse_transform(scaled_data), label='Actual Data', color='blue')
    plt.plot(data.index, train_plot, label='Train Prediction', color='green')
    plt.plot(data.index, test_plot, label='Test Prediction', color='red')
    plt.xlabel('Date')
    plt.ylabel('Stock Price')
    plt.title('Stock Price Prediction using Stacked LSTM')
    plt.legend()
    plt.show()

    # 12. Future Prediction for Next 30 Days
    temp_input = list(scaled_data[-time_step:].flatten())
    future_output = []
    for i in range(30):
        x_input = np.array(temp_input[-time_step:])
        x_input = x_input.reshape(1, time_step, 1)
        yhat = model.predict(x_input, verbose=0)
        future_output.append(yhat[0][0])
        temp_input.append(yhat[0][0])
    future_output = scaler.inverse_transform(np.array(future_output).reshape(-1, 1))
    print("Future 30 Days Prediction:\n", future_output)

    last_date = data.index[-1]
    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30)

    plt.figure(figsize=(14, 7))
    plt.plot(data.index, data['Close'], label='Historical Data', color='blue')
    plt.plot(future_dates, future_output, label='Future Prediction', marker='o', linestyle='--', color='orange')
    plt.xlabel('Date')
    plt.ylabel('Stock Price')
    plt.title('30-Day Future Stock Price Prediction')
    plt.legend()
    plt.show()

    # 13. Plot Sentiment Distribution (if sentiment data exists)
    try:
        sentiment_df = pd.read_csv("stock_sentiment.csv")
        sentiment_counts = sentiment_df['sentiment'].value_counts()
        plt.figure(figsize=(8, 6))
        plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140)
        plt.title('Sentiment Distribution')
        plt.show()
    except Exception as e:
        print("Error reading or plotting sentiment data:", e)

if __name__ == "__main__":
    main()